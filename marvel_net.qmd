---
title: "Social Network Final Assignment"
author: "Gina Tedesco & Gür Piren"
format: pdf
editor: visual
---

## Introduction

Some introduction words

## Dataset Handling

### Libraries and Explore the Dataset

Loading Libraries

--\> I assume we will need all of these

```{r}
#| message: false
#| warning: false
set.seed(123)
library(dplyr)
library(readr)
library(igraph)
library(RColorBrewer)
library(tinytex)
library(ggplot2)
library(knitr)
library(boot)
library(linkprediction)
library(RSpectra)
library(Matrix)
```

### Marvel Universe social network, 2002

The network shows Marvel characters as well as comic book volumes linked with those characters.

The dataset can be accessed through this [link](https://networks.skewed.de/net/marvel_universe)

**Number of nodes:** 19.428

**Number of edges:** 95.497

```{r}
base_path <- "./"


g_marvel <- read_graph(paste0(base_path, "network.gml"), format = "gml")

g_marvel

```

An important characteristic of our network is that it is bipartite. Meaning that it is made up of different types of nodes. The first 6.486 rows correspond to character nodes, while the remaining rows represent the comic books those characters appear in.

The purple nodes are the Marvel characters and comic books they appear in.

The grey lines represent connections between the two type of nodes, a character appearing in a specific comic book.

The dense central cluster suggests that many characters appear in multiple comics and often coappear with others, creating some sort of a hub/interconnected core.

The sparser nodes likely represent characters or comics with few connections. These are likely to be either minor characters or less known comic issues.

```{r}
# for the plot styling
nc <- "#9370DB" 
ec <- "grey80"

# plotting area
par(mfrow = c(1,1), mar = c(1,1,2,1))

# loading the network
g <- read_graph("./network.gml", format = "gml")

# plotting the netwrok

plot(
  g,
  main = "Marvel Universe Network",
  vertex.size = 3,
  vertex.label = NA,
  vertex.color = nc,
  vertex.frame.color = "white",
  edge.color = ec,
  edge.arrow.size = 0
)

```

## Questions & Answers

### 1. Find the theoretical epidemic threshold *βc* for your network for the 


```{r}
library(igraph)
library(Matrix)
library(RSpectra)

# Load the graph
g_marvel <- read_graph("./network.gml", format = "gml")

# Compute the largest eigenvalue of the adjacency matrix efficiently
A <- as_adjacency_matrix(g_marvel, sparse = TRUE)
lambda_max <- eigs(A, k = 1, which = "LM")$values

# Compute the epidemic threshold
beta_c <- 1 / lambda_max

# Output
cat("Theoretical epidemic threshold (βc):", beta_c, "\n")

```

### 2. Assuming that randomly-selected 1% initial spreaders, simulate the SIR model below and above that threshold and plot the number of infected people as a function of *β*.

In the below chunk, we calculate a threshold which helps us determine the point at which an information starts spreading widely in a network instead of dying out. The threshold, in general, tells us how well-spreading an information or contagious a disease needs to be to turn into an epidemic within a network structure.

We set simulation parameters to control the conditions of the experiment and observe how the spreading behaves under different scenarios. This helps us understand the network’s response to varying spreading rates and how close we are to the critical threshold where information starts to travel way more. So, we create a range of 10 different spreading rates (β values), starting from well below the threshold (0.2 times the βc) up to twice the threshold.

The plot shows that when β (information spreading rate) is below around 0.015, the theoretical threshold we acquired prior to plotting, very few people get the word. But once β goes above that threshold, the total number of informed individuals rises sharply, crossing 1.000, and continuing to grow. This means the gossip only spreads widely if β is high enough.

```{r}

g <- read_graph("./network.gml", format = "gml")
N <- vcount(g)

# compute a threshold 
A <- as_adjacency_matrix(g, sparse = TRUE)
lambda_max <- eigs(A, k = 1, which = "LM")$values
beta_c <- 1 / lambda_max

cat("Theoretical epidemic threshold (βc):", beta_c, "\n")

# parameters
gamma <- 1     # recovery rate 
beta_values <- seq(0.2 * beta_c, 2 * beta_c, length.out = 10)  # the range for threshold values
initial_frac <- 0.01  # 1% seed nodes as asked. means that %1 will get infected
T_max <- 20           # for the duration of the simulation

# SIR simulation function
simulate_SIR <- function(g, beta, gamma, initial_frac, T_max) {
  N <- vcount(g)
  state <- rep("S", N)
  names(state) <- V(g)$name
  
  # to randomly infect initial nodes
  initial_infected <- sample(V(g), size = ceiling(initial_frac * N))
  state[initial_infected] <- "I"
  
  infected_count <- numeric(T_max)
  
  for (t in 1:T_max) {
    infected_count[t] <- sum(state == "I")
    new_state <- state
    
    for (i in which(state == "I")) {
      neighbors <- neighbors(g, i)
      for (n in neighbors) {
        if (state[n] == "S" && runif(1) < beta) {
          new_state[n] <- "I"
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    
    state <- new_state
  }
  
  total_infected <- sum(state == "R")
  return(total_infected)
}

# running the simulations across different β values
set.seed(42)
results <- data.frame(
  beta = beta_values,
  infected = sapply(beta_values, function(b) simulate_SIR(g, b, gamma, initial_frac, T_max))
)

# plot 

plot(
  results$beta,
  results$infected,
  type = "b",
  col = "darkred",
  pch = 16,
  xlab = expression(beta),
  ylab = "Total Infected",
  main = "Total Infected vs β (SIR Simulation)"
)
abline(v = beta_c, col = "blue", lty = 2)
legend("bottomright", legend = c("βc threshold"), col = "blue", lty = 2)

```

### 3. Choose a *β* well-above above *βc*. Using centrality, communities or any other suitable metric, find a better set of 1% of seeds in the network so we get more infected people than the random case. Measure the difference of your choice with the random case as:

#### a) The difference in the total number of infected people

The infection rate we used is twice the threshold we acquired earlier, meaning a large outbreak is VERY likely. When starting with random nodes, about 1.527 characters got infected. However, when starting with the most connected characters, 1.760 got infected, and as expected, showing that targeting central nodes spreads the infection more effectively.


```{r}

# computing βc again
A <- as_adjacency_matrix(g, sparse = TRUE)
lambda_max <- eigs(A, k = 1, which = "LM")$values
beta_c <- 1 / lambda_max
beta <- 2 * beta_c  # two times the threshold, as requested in assignment
gamma <- 1          # recovery rate
T_max <- 20         # duration of the simulation
initial_frac <- 0.01
seed_count <- ceiling(N * initial_frac)

cat("Theoretical epidemic threshold (βc):", beta_c, "\n")
cat("Simulating with β =", beta, "\n\n")

# SIR simulation function with custom seed set
simulate_SIR_with_seeds <- function(g, beta, gamma, T_max, seeds) {
  N <- vcount(g)
  state <- rep("S", N)
  state[seeds] <- "I"
  
  for (t in 1:T_max) {
    new_state <- state
    for (i in which(state == "I")) {
      for (n in neighbors(g, i)) {
        if (state[n] == "S" && runif(1) < beta) {
          new_state[n] <- "I"
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    if (sum(new_state == "I") == 0) break
    state <- new_state
  }
  
  return(sum(state == "R"))  # total infected = number of recovered nodes
}

# random seeding
set.seed(123)
random_seeds <- sample(V(g), seed_count)
infected_random <- simulate_SIR_with_seeds(g, beta, gamma, T_max, random_seeds)

# degree centrality-based seeding 
top_degree_nodes <- order(degree(g), decreasing = TRUE)[1:seed_count]
infected_centrality <- simulate_SIR_with_seeds(g, beta, gamma, T_max, top_degree_nodes)

# results
cat("Total infected (random seeding):     ", infected_random, "\n")
cat("Total infected (centrality seeding): ", infected_centrality, "\n")
cat("Difference (centrality - random):    ", infected_centrality - infected_random, "\n")


```

#### b) The difference in the time of the peak of infection (when most infections happen).

Here we try to simulate the process and track how many characters are informed at each time step. The results will return both the total number of characters who got the information and the time step when spreading peaked.


```{r}
simulate_SIR_with_peak <- function(g, beta, gamma, T_max, seeds) {
  N <- vcount(g)
  state <- rep("S", N)
  state[seeds] <- "I"
  
  infected_count <- numeric(T_max)
  
  for (t in 1:T_max) {
    infected_count[t] <- sum(state == "I")
    
    new_state <- state
    for (i in which(state == "I")) {
      for (n in neighbors(g, i)) {
        if (state[n] == "S" && runif(1) < beta) {
          new_state[n] <- "I"
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    if (sum(new_state == "I") == 0) {
      infected_count[(t+1):T_max] <- 0
      break
    }
    state <- new_state
  }
  
  total_infected <- sum(state == "R")
  peak_time <- which.max(infected_count)
  return(list(total = total_infected, peak = peak_time))
}

```

#### Run Simulation

The results shows that when spreading of information started randomly, the peak occurred at time step 7. When it started from central nodes, the peak occurred at time step 2. This means that spreading occurs much faster when it began with highly connected individuals.

```{r}
set.seed(123)
random_seeds <- sample(V(g), seed_count)
res_random <- simulate_SIR_with_peak(g, beta, gamma, T_max, random_seeds)

# using centrality for seeding
top_degree_nodes <- order(degree(g), decreasing = TRUE)[1:seed_count]
res_centrality <- simulate_SIR_with_peak(g, beta, gamma, T_max, top_degree_nodes)

# comparison
cat("Peak time (random seeding):     ", res_random$peak, "\n")
cat("Peak time (centrality seeding): ", res_centrality$peak, "\n")
cat("Difference in peak time:        ", res_centrality$peak - res_random$peak, "\n")

```

### 4. Using the same *β*, design a “quarantine strategy”: at time step *t* = 3 or 4 , quarantine 20% of the susceptible population. You can model quarantine by temporally removing these nodes. Release the quarantined nodes time steps later, making them susceptible again. Measure the difference with respect to no quarantine.

As asked, we define a function that simulates the spread of an epidemic while introducing a quarantine. At a chosen time step (we pick 3), it temporarily removes 20% of the healthy population by marking them as quarantined so they can’t get infected. After a delay of 3 steps, we put them back into the simulation as susceptible nodes again. We track how the infection spreads and returns the total number infected and the time the infection peaked.

```{r}
simulate_SIR_with_quarantine <- function(g, beta, gamma, T_max, seeds,
                                         quarantine_time = 3, release_delay = 3, quarantine_frac = 0.20) {
  N <- vcount(g)
  state <- rep("S", N)
  state[seeds] <- "I"
  
  infected_count <- numeric(T_max)
  quarantine_active <- FALSE
  quarantine_nodes <- c()
  
  for (t in 1:T_max) {
    infected_count[t] <- sum(state == "I")
    
    # let's start the quarantine 
    if (t == quarantine_time) {
      sus_ids <- which(state == "S")
      quarantine_nodes <- sample(sus_ids, size = ceiling(quarantine_frac * length(sus_ids)))
      state[quarantine_nodes] <- "Q"  # quarantined 
      quarantine_active <- TRUE
    }
    
    # ending the quarantine 
    if (t == quarantine_time + release_delay && quarantine_active) {
      state[quarantine_nodes] <- "S"
      quarantine_active <- FALSE
    }
    
    new_state <- state
    for (i in which(state == "I")) {
      for (n in neighbors(g, i)) {
        if (state[n] == "S" && runif(1) < beta) {
          new_state[n] <- "I"
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    
    if (sum(new_state == "I") == 0) {
      infected_count[(t+1):T_max] <- 0
      break
    }
    
    state <- new_state
  }
  
  total_infected <- sum(state == "R")
  peak_time <- which.max(infected_count)
  
  return(list(total = total_infected, peak = peak_time))
}

```

#### Run it with and wihtout quarantine

In below chunk, we run two simulations of the epidemic using the same initial infected nodes. The first simulation has no quarantine (quarantine starts very late and affects nobody). The second applies quarantine at time step 3, removing 20% of healthy nodes for 3 steps. We then compare the total number of people infected in each case.

The results show that without quarantine, 1.601 got infected, while with quarantine, only 1.211 got infected. This means a reduction of 390 cases, indicating that quarantining part of the population early slowed the spread and lowered total infections.

```{r}
set.seed(42)
random_seeds <- sample(V(g), seed_count)

# no quarantine - simulation
res_no_q <- simulate_SIR_with_quarantine(g, beta, gamma, T_max, random_seeds,
                                         quarantine_time = 100, release_delay = 0, quarantine_frac = 0)

# scenario with quarantine starting at t = 3, 20% quarantined, released after 3 steps
res_q <- simulate_SIR_with_quarantine(g, beta, gamma, T_max, random_seeds,
                                      quarantine_time = 3, release_delay = 3, quarantine_frac = 0.20)

# results
cat("Total infected (no quarantine):    ", res_no_q$total, "\n")
cat("Total infected (with quarantine): ", res_q$total, "\n")
cat("Difference in infections:         ", res_q$total - res_no_q$total, "\n")

```

### 5. Suppose now that you can convince 5% of people in the network not to spread that information at all.


#### - Choose those 5% randomly in the network. Simulate the SIR model above βc using 1% of the remaining nodes as seeds. Choose those seeds randomly.

Here we randomly select 5% of nodes as non-spreaders who can get infected but won’t infect others either. The function then seeds the infection only among the remaining spreaders and runs the SIR simulation accordingly. The result tracks total infections and peak infection time, showing how having non-spreaders mightt affect the epidemic.

```{r}
simulate_SIR_with_nonspreaders <- function(g, beta, gamma, T_max, nonspreaders_frac = 0.05, seed_frac = 0.01) {
  N <- vcount(g)
  nodes <- V(g)
  
  # picking non-spreaders randomly
  nonspreaders <- sample(nodes, size = ceiling(nonspreaders_frac * N))
  spreaders <- setdiff(nodes, nonspreaders)
  
  # picking 1% of spreaders as seeds
  seed_count <- ceiling(seed_frac * length(spreaders))
  seeds <- sample(spreaders, seed_count)
  
  # initializing SIR states
  state <- rep("S", N)
  state[seeds] <- "I"
  
  infected_count <- numeric(T_max)
  
  for (t in 1:T_max) {
    infected_count[t] <- sum(state == "I")
    new_state <- state
    
    for (i in which(state == "I")) {
      # only spread if not in the non-spreader group
      if (!(i %in% nonspreaders)) {
        for (n in neighbors(g, i)) {
          if (state[n] == "S" && runif(1) < beta) {
            new_state[n] <- "I"
          }
        }
      }
      # recovery
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    
    if (sum(new_state == "I") == 0) {
      infected_count[(t+1):T_max] <- 0
      break
    }
    
    state <- new_state
  }
  
  total_infected <- sum(state == "R")
  peak_time <- which.max(infected_count)
  
  return(list(total = total_infected, peak = peak_time))
}

```

#### Run the simulation

The result shows that with 5% non-spreaders, a total of 1.428 nodes got infected during the epidemic. The peak time was 7, meaning the highest number of infections happened at time step 7. This indicates that having non-spreaders slowed the spread compared to no intervention, but the timing of the epidemic’s peak stayed the same.

These results also have interesting implications compared to the quarantine scenario we practiced earlier. The fact that convincing 5% not to spread the news reduces infections more than no quarantine scenario, but less than quarantining 20% of the network suggests that larger, temporary removal (quarantine) is more effective at slowing the spread than a smaller fraction permanently refusing to spread. 

It implies that quarantining a bigger group (or isolating in the information-spreading case), even temporarily, can have a stronger impact than a smaller group who never spreads. This highligts the importance of the proportion and timing of interventions in controlling epidemics.

```{r}
set.seed(42)
res_random_blockers <- simulate_SIR_with_nonspreaders(g, beta, gamma, T_max)

cat("Total infected (5% random non-spreaders): ", res_random_blockers$total, "\n")
cat("Peak time:                                ", res_random_blockers$peak, "\n")

```

#### - Choose those 5% according to their centrality. Simulate the SIR model above βc using 1% of the remaining nodes as seeds. Choose those seeds randomly.

Next, we select the top 5% of nodes by centrality as non-spreaders who cannot transmit the information The function randomly picks 1% of the remaining nodes as seeds to start the spreading The SIR simulation runs with these settings, tracking total infected and peak infection time.

```{r}
simulate_SIR_with_nonspreaders_central <- function(g, beta, gamma, T_max,
                                                    nonspreaders_frac = 0.05,
                                                    seed_frac = 0.01,
                                                    centrality_func = degree) {
  N <- vcount(g)
  nodes <- V(g)
  
  # choosing top 5% by centrality as non-spreaders
  centrality_scores <- centrality_func(g)
  top_ids <- order(centrality_scores, decreasing = TRUE)[1:ceiling(nonspreaders_frac * N)]
  nonspreaders <- nodes[top_ids]
  spreaders <- setdiff(nodes, nonspreaders)
  
  # randomly selecting 1% of spreaders as seeds
  seed_count <- ceiling(seed_frac * length(spreaders))
  seeds <- sample(spreaders, seed_count)
  
  # initializig SIR states
  state <- rep("S", N)
  state[seeds] <- "I"
  
  infected_count <- numeric(T_max)
  
  for (t in 1:T_max) {
    infected_count[t] <- sum(state == "I")
    new_state <- state
    
    for (i in which(state == "I")) {
      if (!(i %in% top_ids)) {
        for (n in neighbors(g, i)) {
          if (state[n] == "S" && runif(1) < beta) {
            new_state[n] <- "I"
          }
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    
    if (sum(new_state == "I") == 0) {
      infected_count[(t+1):T_max] <- 0
      break
    }
    
    state <- new_state
  }
  
  total_infected <- sum(state == "R")
  peak_time <- which.max(infected_count)
  
  return(list(total = total_infected, peak = peak_time))
}

```

#### Run the simulation with degree centrality

We finally run the SIR simulation where the top 5% most connected nodes are non-spreaders as explained earlier, so they don’t transmit infection. The infection starts from 1% of the other nodes.

The result shows a very low total infected count (221) and an early peak at time 1, meaning blocking highly connected nodes drastically limits the spread and the spreading peaks almost immediately with few infections. This has good implications for epidemics!

```{r}
set.seed(42)
res_central_blockers <- simulate_SIR_with_nonspreaders_central(g, beta, gamma, T_max)

cat("Total infected (5% top-degree non-spreaders): ", res_central_blockers$total, "\n")
cat("Peak time:                                    ", res_central_blockers$peak, "\n")

```

#### - Measure the difference between both cases as you did in step 3.

The negative difference in total infected (-1207) means that choosing the top 5% by centrality as non-spreaders greatly reduced infections compared to random non-spreaders.

The peak time difference of -6 shows the epidemic peaked 6 time steps earlier with centrality-based blockers, indicating the outbreak was much shorter and more contained.


```{r}
diff_total_infected <- res_central_blockers$total - res_random_blockers$total
cat("Difference in total infected (centrality - random):", diff_total_infected, "\n")

diff_peak_time <- res_central_blockers$peak - res_random_blockers$peak
cat("Difference in peak time (centrality - random):", diff_peak_time, "\n")

```

### 6. Comment on the relationship between the findings in steps 3 and 5 using the same type of centrality for the 1% in step 3 and 5% in step 5.

The findings show that targeting the most central nodes both for initial infection seeds and for blocking spreaders has a big impact on the epidemic. In the former, seeding infection in top central nodes caused an earlier and sharper peak, while in in the latter, blocking those same central nodes greatly reduced total infections and shortened the outbreak. This highlights, expectedly, how central nodes play a crucial role in spreading, so focusing interventions on them can either speed up or effectively slow down an epidemic.

### 7. With the results of step 2, train a model that predicts that time to infection of a node using their degree, centrality, betweeness, page rank and any other predictors you see fit. Use that model to select the seed nodes as those with the smallest time to infection in step 3. Repeat step 5 with this knowledge.



The below model attempts to predict each node's time to infection using degree, betweenness, PageRank, and eigenvector centrality. However, it performs poorly. The R-squared is only 0.0048, meaning the model explains less than 0.5% of the variation in infection time. The overall p-value from the F-test is 0.10, suggesting the model is not statistically significant.

Looking at the individual predictors, none of them are significant at the desired level. Only eigenvector centrality is significant, compared to other methods, with a p-value of 0.0586, indicating a weak negative relationship with time to infection. 

The other variables—degree, betweenness, and PageRank—show no meaningful predictive power. This suggests that these centrality features alone are not strong predictors of when a node will get infected in the simulation.


```{r}

# computing node features
features_df <- data.frame(
  node = V(g)$name,
  degree = degree(g),
  betweenness = betweenness(g),
  pagerank = page_rank(g)$vector,
  eigen = eigen_centrality(g)$vector
)

# capture infection times from step 2
# update the simulation function from earlier to store per-node infection time. necessaryt

simulate_SIR_with_infection_time <- function(g, beta, gamma, T_max, seeds) {
  N <- vcount(g)
  state <- rep("S", N)
  infection_time <- rep(NA, N)
  state[seeds] <- "I"
  infection_time[seeds] <- 0
  
  for (t in 1:T_max) {
    new_state <- state
    for (i in which(state == "I")) {
      for (n in neighbors(g, i)) {
        if (state[n] == "S" && runif(1) < beta) {
          new_state[n] <- "I"
          infection_time[n] <- t
        }
      }
      if (runif(1) < gamma) {
        new_state[i] <- "R"
      }
    }
    if (sum(new_state == "I") == 0) break
    state <- new_state
  }
  
  return(infection_time)
}

# let's simulate and train model
set.seed(42)
seed_count <- ceiling(0.01 * vcount(g))
seeds <- sample(V(g), seed_count)
inf_time <- simulate_SIR_with_infection_time(g, beta, gamma, T_max, seeds)

# adding target variable to features
features_df$inf_time <- inf_time
df_model <- na.omit(features_df)  # drop nodes never infected

# training model
model <- lm(inf_time ~ degree + betweenness + pagerank + eigen, data = df_model)
summary(model)

# predicting infection time for all nodes
features_df$predicted_time <- predict(model, newdata = features_df)

# use predicted time:

# Step 3 redo: top 1% with smallest predicted infection time as seeds

# Step 5 redo: top 5% with smallest predicted infection time as blockers

# seeding set
seeding_nodes <- V(g)[order(features_df$predicted_time)][1:seed_count]

# blocking set:
block_count <- ceiling(0.05 * vcount(g))
blocking_nodes <- V(g)[order(features_df$predicted_time)][1:block_count]

```

## Conclusion

Our analysis explored how targeted interventions based on network centrality affect the spread of an epidemic using the SIR model. We found that blocking the top 5% most central nodes by degree significantly reduced total infections and delayed the peak compared to blocking nodes at random, highlighting the importance of network structure in containment strategies.

When comparing centrality-based seed selection to random seeding, we observed that targeting highly central nodes as initial spreaders led to faster and broader epidemics. 

Attempts to predict time to infection using centrality measures showed limited success, as our linear model explained little variance, suggesting that infection dynamics are influenced by more than just static node features. 

Overall, our results show that network-informed strategies can be effective for both slowing and accelerating epidemics or spreading, but predicting exact infection timings remains a challenge for the scope of this assignmet.


## References



